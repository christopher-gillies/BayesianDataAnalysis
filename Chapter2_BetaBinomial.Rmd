---
title: "Chapter2_BetaBinomial"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
require(ggplot2)
```

# R Beta Distribution

### Beta distribution
\begin{equation}
\text{Beta}(\theta; \alpha, \beta) = \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha) \Gamma(\beta)} \theta^{\alpha -1} (1 - \theta)^{\beta - 1}
\end{equation}

In R this function is $dbeta(\alpha,\beta)$.


### Beta function
\begin{equation}
B(\alpha,\beta) = \int_0^1 \theta^{\alpha -1} (1 - \theta)^{\beta - 1} d \theta = \frac{\Gamma(\alpha) \Gamma(\beta)}{\Gamma(\alpha + \beta)}
\end{equation}


#### Expected Value of Beta distribution

\begin{equation}
E[\theta] = \int_0^1 \theta \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha) \Gamma(\beta)} \theta^{\alpha -1} (1 - \theta)^{\beta - 1} d \theta = \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha) \Gamma(\beta)} \int_0^1 \theta \theta^{\alpha -1} (1 - \theta)^{\beta - 1} d \theta = 
\frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha) \Gamma(\beta)} \int_0^1 \theta^{\alpha} (1 - \theta)^{\beta - 1} d \theta
\end{equation}

Where $\Gamma(n) = (n-1)!$

\begin{equation}
E[\theta] = \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha) \Gamma(\beta)} \frac{\Gamma(\alpha + 1) \Gamma(\beta)}{\Gamma(\alpha + \beta + 1)} = \frac{\alpha}{\alpha + \beta}
\end{equation}

#### Variance of Beta

\begin{equation}
Var[\theta] = \frac{\alpha\beta}{(\alpha + \beta)^2 (\alpha + \beta + 1)}
\end{equation}

```{r}
n = 1000
p1 = rbeta(n,1,1)
p2 = rbeta(n,25,25)
p3 = rbeta(n,5,7)
p4 = rbeta(n,7,2)
p = c(p1,p2,p3,p4)
lab = factor(c(rep("1,1",n),rep("25,25",n),rep("5,7",n),rep("7,2",n)))
ggplot(data.frame()) + geom_density(aes(x=p,color=lab))
```
In R this function is $beta(\alpha,\beta)$.

# Fitting a beta distribution using a normal approximation.

Using an $Beta(\theta,7,2)$, let us try to fit a normal distirbution to it.

```{r}

n = 1000
p = rbeta(n,10,2)
mu = mean(p)
v = var(p)
x = rnorm(n,mean = mu, sd = sqrt(v))

p_trans = log(p / (1-p) )
mu_p_trans = mean(p_trans)
v_p_trans = var(p_trans)

x2 = rnorm(n,mean = mu_p_trans, sd = sqrt(v_p_trans))
x2_untrans = exp(x2) / (1 + exp(x2))

vals = c(p,x,x2_untrans)
lab = c(rep("b 10 ,2",n),rep("normal",n), rep("normal log(p / (1 - p))",n))
ggplot(data.frame()) + geom_density(aes(x=vals,color=lab))

```

The normal approximation using the logit transform $log(p / (1-p))$ works better than just doing a simple normal transformation.

## Binomial likelihood with Beta prior over $\theta$ has a Beta posterior over $\theta$

A binomial likelihood with $y$ successes from $n$ trials is proportional to
\begin{equation}
p(y | \theta) \propto \theta^{y} (1 - \theta)^{n -y}
\end{equation}

The prior of $\theta$ is proportional to

\begin{equation}
p(\theta) \propto \theta^{\alpha - 1} (1 - \theta) ^{\beta - 1}
\end{equation}

which is a beta distribution with parameters $\alpha$ and $\beta$: $\theta \sim \text{Beta}(\alpha,\beta)$. This suggests the prior density corresponds to $\alpha -1$ successes and $\beta - 1$ failures.

The postierior distribution $p(\theta | y)$ is proportional to
\begin{equation}
p(\theta | y) \propto \theta^{y} (1 - \theta)^{n -y} \theta^{\alpha - 1} (1 - \theta) ^{\beta - 1} = \theta^{y + \alpha - 1} (1 - \theta)^{n - y + \beta - 1}
\end{equation}

The posterior distribution is thus represented by $\text{Beta}(\theta | \alpha + y, \beta + n - y)$


### Example of how posterior changes as we get more evidence

```{r}

# prior
alpha = 3
beta = 3

beta_llh = function(theta,alpha,beta) {
  theta^(alpha - 1) * (1 - theta)^(beta - 1)
}

thetas = seq(0,1,0.01)
df.prior = data.frame(llh = beta_llh(thetas,alpha,beta),theta=thetas)
ggplot(df.prior) + geom_line(aes(x=theta,y=llh)) + ggtitle("Prior distribution: 2 success, 2 failure")

data = rbinom(20,prob = 0.7, size = 1)
successes = 0
failures = 0
for(d in data) {
  if(d == 1) {
    successes = successes + 1
    alpha = alpha + 1
  } else {
    failures = failures + 1
    beta = beta + 1
  }
  df.posterior = data.frame(llh = beta_llh(thetas,alpha,beta),theta=thetas)
  gplot = ggplot(df.posterior) + geom_line(aes(x=theta,y=llh)) + ggtitle(paste0("Posterior after: ",successes," successes, and failures = ", failures))
  print(gplot)
}
df.ml = data.frame(llh = beta_llh(thetas,successes + 1,failures + 1),theta=thetas)
 ggplot(df.ml) + geom_line(aes(x=theta,y=llh)) + ggtitle("ML Estimator")
```


### Biased Coin Example
In this example, we will start with the assumption that the coin is fair and keep flipping the coin until we can declare it to be biased. We can do this by integrating the posterior distribution and computing $p(\theta > 0.5 | y) > 0.99$. We can use either numerical integration or simulation.

Compute the probability of positive association (PPA).
\begin{equation}
\text{Posterior odds (PO)} = \text{BF} \times \text{Prior odds (PrO)}
\end{equation}

\begin{equation}
PO = \frac{\int_0^1 \theta^{y + \alpha - 1} (1 - \theta)^{n - y + \beta - 1} d \theta}{\theta_0^{y + \alpha - 1} (1 - \theta_0)^{n - y + \beta - 1}} \times \text{PrO}
\end{equation}

If $\theta_0 = 0.5$, then

\begin{equation}
BF = \frac{\int_0^1 \theta^{y + \alpha - 1} (1 - \theta)^{n - y + \beta - 1} d \theta}{0.5^{y + \alpha - 1} (0.5)^{n - y + \beta - 1}} =\frac{\int_0^1 \theta^{y + \alpha - 1} (1 - \theta)^{n - y + \beta - 1} d \theta}{0.5^{n + \alpha + \beta - 2}}
\end{equation}

\begin{equation}
PO = \frac{PPA}{1 - PPA} \Rightarrow PO (1 - PPA) = PPA \Rightarrow PPA = PO - PO \times PPA  \Rightarrow PPA + PPA \times PO - PO 
\end{equation}

\begin{equation}
\Rightarrow PPA ( 1 + PO) = PO \Rightarrow PPA = \frac{PO}{1 + PO}
\end{equation}

```{r}
bf = function(alpha, beta,alt_p=0.5) {
  beta(alpha,beta) / (alt_p^(alpha + beta - 2))
}

ppa = function(bf,prior_odds=1) {
  posterior_odds=bf*prior_odds
  posterior_odds / (1 + posterior_odds)
}
```

```{r}

p = 0.6
alpha = 11
beta = 11
n = 200

data = rbinom(n,prob = p,size = 1)
s = 0
f = 0
ps_g_0_5 = c( pbeta(0.5,alpha,beta,lower.tail=F))
ppas = c(ppa(bf(alpha,beta)))
for( d in data) {
  if(d == 1){
    alpha = alpha + 1
  } else {
    beta = beta + 1
  }
  ppas = c(ppas, ppa(bf(alpha,beta)))
  ps_g_0_5 = c(ps_g_0_5, pbeta(0.5,alpha,beta,lower.tail=F))
}

ggplot(data.frame(p=ps_g_0_5,n=0:n)) + geom_line(aes(x=n,y=p)) + geom_abline(slope = 0, intercept = 0.99) + ggtitle("Prob theta > 0.5")

ggplot(data.frame(p=ppas,n=0:n)) + geom_line(aes(x=n,y=p)) + geom_abline(slope = 0, intercept = 0.99) + ggtitle("PPA > 0.5")

theta = seq(0,1,0.01)
y = dbeta(theta,alpha,beta)
ggplot(data.frame(theta=theta,prob=y))+ geom_line(aes(x=theta,y=prob)) + ggtitle("Posterior density")
```


