---
title: "Normal Likelihood Ratio"
author: "Christopher Gillies"
date: "2/8/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Model
We want to compare the likelihood of two models. One were we assume the means are the same, and the other where we assume the means are different. We  assume the variance within each group is different but known.

Normal distribution density function
\begin{equation}
f(x|\mu,\sigma^2) = \frac{1}{\sqrt{2 \pi \sigma^2}} \exp \left [ -\frac{(x-\mu)^2}{2\sigma^2} \right ]
\end{equation}

$H_0: \mu_0 = \mu_1$

$H_1: \mu_0 \ne \mu_1$



Assume we have $n_0$ observations in group 0 and $n_1$ observations in group 1 and given the group status the observations are IID with the same variance
.
Under $H_0$ we have the following likelihood:

\begin{equation}
L(Y|H_0) = \left ( \frac{1}{\sqrt{2 \pi \sigma^2}} \right )^{n_0}  \left ( \frac{1}{\sqrt{2 \pi \sigma^2}} \right )^{n_1} \exp \left [ -\sum_{i=0}^{n_0}\frac{(Y_{0i}- \hat \mu_{0+1})^2}{2 \sigma^2} \right ] \exp \left [ -\sum_{i=0}^{n_1}\frac{(Y_{1i}- \hat \mu_{0+1})^2}{2  \sigma^2} \right ]
\end{equation}

\begin{equation}
L(Y|H_0) = \left ( \frac{1}{\sqrt{2 \pi \sigma^2}} \right )^{n}  \exp \left [ -\sum_{i=0}^{n}\frac{(Y_{i}- \hat \mu_{0+1})^2}{2 \sigma^2} \right ]
\end{equation}

where $Y_{0i}$ is the value of the $i$th example from group 0, the same for $Y_{1i}$, $\sigma^2$ is the variance is the overall variance and  $\mu_{0+1}$ is the pooled mean estimate.

\begin{equation}
L(Y|H_1) = \left ( \frac{1}{\sqrt{2 \pi \sigma^2}} \right )^{n_0} \left ( \frac{1}{\sqrt{2 \pi  \sigma^2}} \right )^{n_1} \exp \left [ -\sum_{i=0}^{n_0}\frac{(Y_{0i}- \hat \mu_{0})^2}{2  \sigma^2} \right ] \exp \left [ -\sum_{i=0}^{n_1}\frac{(Y_{1i}- \hat \mu_{1})^2}{2 \sigma^2} \right ]
\end{equation}

\begin{equation}
L(Y|H_1) = \left ( \frac{1}{\sqrt{2 \pi \sigma^2}} \right )^{n} \exp \left [ -\sum_{i=0}^{n_0}\frac{(Y_{0i}- \hat \mu_{0})^2}{2  \sigma^2} \right ] \exp \left [ -\sum_{i=0}^{n_1}\frac{(Y_{1i}- \hat \mu_{1})^2}{2 \sigma^2} \right ]
\end{equation}

where $\hat \mu_{0}$ is the MLE of the mean of group 0, and $\hat \mu_{1}$ is the MLE of the mean of group 1.

\begin{equation}
\frac{ L(Y|H_0) }{L(Y|H_1)} = \frac {  \exp \left [  \frac{-1}{2\sigma^2} \sum_{i=0}^{n}(Y_{i}- \hat \mu_{0+1})^2 \right ] } { \exp \left [ \frac{-1}{2\sigma^2} \sum_{i=0}^{n_0}(Y_{0i}- \hat \mu_{0})^2 \right ] \exp \left [ \frac{-1}{2\sigma^2} \sum_{i=0}^{n_1}(Y_{1i}- \hat \mu_{1})^2 \right ] } = \frac{ \exp( \frac{-n \hat \sigma_{0+1}^2}{2\sigma^2} ) } {  \exp(\frac{-n_0 \hat \sigma_{0}^2 - n_1 \hat \sigma_{1}}{2\sigma^2})  }
\end{equation}


Taking the log we have
\begin{equation}
-2 \log \left ( \frac{ L(Y|H_0) }{L(Y|H_1)} \right ) = -2 \left ( \frac{n_0 \hat \sigma_{0}^2 + n_1 \hat \sigma_{1}}{2\sigma^2} - \frac{n \hat \sigma_{0+1}^2}{2\sigma^2} \right ) = \frac{-1}{\sigma^2} \left ( n_0 \hat \sigma_{0}^2 + n_1 \hat \sigma_{1} - n \hat \sigma_{0+1}^2 \right )
\end{equation}

We can estimate $\sigma^2$ using a pooled variance estimate $S_p^2$.


\begin{equation}
S_p^2=\frac{ (n_0 - 1) S_0^2 + (n_1 - 1) S_1^2} { n_0 + n_1 - 2}
\end{equation}

where $S_0^2$ and $S_1^2$ are sample variance estiamtes for group 0 and group 1.

\begin{equation}
-2 \log \left ( \frac{ L(Y|H_0) }{L(Y|H_1)} \right ) \sim \chi^2_1
\end{equation}


```{r }
pop.var <- function(x) var(x) * (length(x)-1) / length(x)

pooled.var <- function(s0,s1) {
  s0.var = var(s0)
  s1.var = var(s1)
  
  n0 = length(s0)
  n1 = length(s1)
  
  ( (n0 - 1) * s0.var + (n1 - 1) * s1.var ) / (n0 + n1 - 2)
}

lrt = function(s0,s1) {
  pooled = c(s0,s1)
  n = length(pooled)
  n0 = length(s0)
  n1 = length(s1)
  pooled.var.est = pooled.var(s0,s1)
  
  chi =  -1/pooled.var.est * ( n0 * pop.var(s0) + n1 * pop.var(s1) - n * pop.var(pooled)  )
  pchisq(chi,df=1,lower.tail=F)
}

lrt.simple = function(s0,s1) {
  pooled = c(s0,s1)
  pooled.mean = mean(pooled)
  pooled.var.est = pooled.var(s0,s1)
  s0.mean = mean(s0)
  s1.mean = mean(s1)
  
  chi = -2 * (sum(dnorm(pooled,mean=pooled.mean,sd=sqrt(pooled.var.est),log = T)) - 
    sum(dnorm(s0,mean=s0.mean,sd=sqrt(pooled.var.est),log = T)) - sum(dnorm(s1,mean=s1.mean,sd=sqrt(pooled.var.est),log = T)))
  pchisq(chi,df=1,lower.tail=F)
}

```

## Compare LRT simple verus formula
```{r }
n0 = 100
n1 = 100
s0 = rnorm(n0,sd=2)
s1 = rnorm(n1,sd=2)
lrt.simple(s0,s1)
lrt(s0,s1)

```


## Check Type I Error

```{r }

p_lrts = c()
p_ts = c()
nsim = 1000
n0 = 100
n1 = 100
p_lms = c()
for(i in seq(1,nsim)) {
  s0 = rnorm(n0,sd=2)
  s1 = rnorm(n1,sd=2)
  y = c(s0,s1)
  x = c( rep(1,length(s0)), rep(0,length(s1)) )
  p_ts = c(p_ts,  t.test(s0,s1,var.equal = TRUE)$p.val)
  p_lrts = c(p_lrts,  lrt.simple(s0,s1)) 
  null = lm(y ~ 1)
  alt = lm(y ~ x)
  p_lms = c(p_lms, anova(null,alt,test="Chisq")[2,5])
}

sum(p_ts < 0.05) / nsim
sum(p_lms < 0.05 ) / nsim
sum(p_lrts < 0.05) / nsim
```


## Check Power

```{r }

p_lrts = c()
p_ts = c()
nsim = 1000
n0 = 100
n1 = 100
p_lms = c()
for(i in seq(1,nsim)) {
  s0 = rnorm(n0)
  s1 = rnorm(n1,mean=0.2)
  y = c(s0,s1)
  x = c( rep(1,length(s0)), rep(0,length(s1)) )
  p_ts = c(p_ts,  t.test(s0,s1)$p.val)
  p_lrts = c(p_lrts,  lrt(s0,s1)) 
  null = lm(y ~ 1)
  alt = lm(y ~ x)
  p_lms = c(p_lms, anova(null,alt,test="Chisq")[2,5])
}

sum(p_ts < 0.05) / nsim
sum(p_lms < 0.05 ) / nsim
sum(p_lrts < 0.05) / nsim
```

So we did all that stuff and got the same thing as a t-test and linear regression! What was the point? To learn how to do a likelihood ratio test so that we can then do a Bayes factor.

# Bayes Factor Based Approach

\[
\frac{p(H_1 | y)}{p(H_0 | y)} = \frac{p(y|H_1) }{p(y|H_0) } \frac{p (H_1 )}{p (H_0)}
\]

The posterior odds = Bayes Factor $\times$ prior odds. The Bayes Factor is $\frac{p(y|H_1) )}{p(y|H_0) }$.


To compute the Bayes Factor we need $p(y|H_0)$ and $p(y|H_1)$, where these are the probabilities of the data given the corresponding models.

\begin{equation}
p(y) = \int p(y|\theta) p(\theta) d\theta
\end{equation}

This integral can be challenging to compute. But if we can compute the normalizing constant for the posterior distribution ($p(\theta|y)$), the we can compute $p(y)$ without the need to compute the integral.

\begin{equation}
p(\theta | y) \propto p(y|\theta) p(\theta) = p(y,\theta)
\end{equation}

\begin{equation}
p(\theta | y) p(y) =  p(y,\theta) =  p(y|\theta) p(\theta)
\end{equation}

\begin{equation}
 p(y) = \frac{p(y|\theta) p(\theta)}{p(\theta | y)}
\end{equation}

So if we know the posterior distribution of $\theta$ given $y$, then we can compute $p(y)$ using any value of $\theta$.

## Normal prior
Assume we have $n$ observations, and the prior distribution of $\theta$ is $N(\theta;\mu_0,\tau^2_0)$. 
\begin{equation}
p(\theta | y) \propto p(\theta)p(y|\theta) = p(\theta) \prod_{i=1}^{n} p(y_i|\theta)
\end{equation}

\begin{equation}
 \propto \exp \left ( -\frac{1}{2\tau_0^2} (\theta-\mu_0)^2   \right ) \exp \left ( \prod_{i=1}^n ( -\frac{1}{2\sigma^2} (y_i-\theta)^2   \right )
\end{equation}

\begin{equation}
 \propto \exp \left ( -\frac{1}{2} \left ( \frac{1}{\tau_0^2} (\theta-\mu_0)^2 + \frac{1}{\sigma^2} \sum_{i=1}^n(y_i-\theta)^2 \right )  \right ) 
\end{equation}

It can be shown that
\[
  p(\theta | y_1, ..., y_n) = p(\theta | \bar y) = N(\theta | \mu_n, \tau_n^2)
\]

\[
  \mu_n =  \frac{\frac{1}{\tau_0^2}\mu_0 + \frac{n}{\sigma^2}\bar y}{ \frac{1}{\tau_0^2} + \frac{n}{\sigma^2}}
\]

\[
\frac{1}{\tau_n^2} = \frac{1}{\tau_0^2} + \frac{n}{\sigma^2}
\]

### Verify they ways we can compute $p(y)$

\[
p(y) = \frac{p(y|\theta) p(\theta)}{p(\theta | y)} = \int p(y|\theta) p(\theta) d\theta
\]

#### Code to compute the likelihood and posterior
```{r}

likelihood = function(y,known_mean,known_var,log=FALSE) {
  compute = function(mu) {
    res = dnorm(y,mean=mu,sd = sqrt(known_var),log=log)
    if(!log) {
      prod(res)
    } else {
      sum(res)
    }
  }
  
  #if there are multiple means return a likelihood for each 1
  if(length(known_mean) > 1) {
    sapply(known_mean, function(mu) {
      compute(mu)
    })
  } else {
    compute(known_mean)
  }
}

prior = function(mu,prior_mean=0,prior_var=1,log=FALSE) {
  dnorm(mu,prior_mean,sqrt(prior_var),log=log)
}


joint = function(y,mu,known_var,prior_mean,prior_var,log=FALSE) {
  if(!log) {
    likelihood(y,mu,known_var,log=log) * prior(mu,prior_mean,prior_var,log)
  } else {
    likelihood(y,mu,known_var,log=log) + prior(mu,prior_mean,prior_var,log)
  }
}

posterior = function(y,mu,known_var,prior_mean,prior_var,log=FALSE) {
  mean_n = 1/prior_var * prior_mean + length(y) / known_var * mean(y)
  mean_n = mean_n / (1/prior_var + length(y)/known_var) 
  sd_n = sqrt( 1/(1/prior_var + length(y)/known_var)  )
  dnorm(mu,mean=mean_n,sd=sd_n,log=log)
}

```

#### Example
The posterior and joint should look the same and they should have a constant proportion between them so we can compute $p(y)$ using any value of $\theta$.
```{r }
joint(y=1,mu=c(-2,-1,0,1,2),1,0,1)
posterior(y=1,mu=c(-2,-1,0,1,2),1,0,1)

joint(y=1,mu=c(-2,-1,0,1,2),1,0,1) / posterior(y=1,mu=c(-2,-1,0,1,2),1,0,1)

mus = seq(-10,10,0.01)
lh = joint(y=c(0,1,1,1,1),mu=mus,1,0,1)
plot( mus,lh,type="l")

mus = seq(-10,10,0.01)
lh = posterior(y=c(0,1,1,1,1),mu=mus,1,0,1)
plot( mus,lh,type="l")


#THIS SHOULD BE 1
integrate(function(mu) {
  posterior(y=c(0,1,1,0,1),mu=mu,1,0,1)
},-10,10)

#THESE SHOULD EQUAL
integrate(function(mu) {
  joint(y=c(0,1,1,0,1),mu=mu,1,0,1)
},-10,10)

joint(y=c(0,1,1,0,1),mu=1,1,0,1) / posterior(y=c(0,1,1,0,1),mu=1,1,0,1)
```